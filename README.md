Perceptron Multi-Couches (MLP)

## ğŸ“ Objectif du projet

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du contrÃ´le continu de notre cours d'apprentissage automatique. L'objectif principal est d'implÃ©menter **un rÃ©seau de neurones Ã  une couche cachÃ©e (MLP)** Ã  partir de zÃ©ro, sans utiliser de bibliothÃ¨ques de deep learning (comme PyTorch ou TensorFlow), afin de comprendre les bases de la rÃ©tropropagation et de l'apprentissage.


## ğŸ“ Contenu du projet

- ImplÃ©mentation d'un **classificateur linÃ©aire manuel** 
- EntraÃ®nement d'une **rÃ©gression logistique avec scikit-learn** 
- ImplÃ©mentation dâ€™un **rÃ©seau de neurones MLP** avec :
  - Une couche cachÃ©e
  - Fonctions d'activation sigmoid et softmax
  - Apprentissage par descente de gradient (backpropagation)
- Comparaison des performances avec :
  - Perceptron & ADALINE 
  - RÃ©gression logistique 
  - MLP de sklearn
  - MLP implÃ©mentÃ© manuellement
- Application finale sur un **jeu de donnÃ©es rÃ©el : MNIST** 

## ğŸ” Dataset utilisÃ©

- `make_moons` : Dataset non-linÃ©aire pour la classification binaire
- `digits` : Dataset MNIST simplifiÃ© inclus dans `sklearn.datasets` pour la classification multi-classes

## ğŸ“Š RÃ©sultats observÃ©s

- Le MLP implÃ©mentÃ© manuellement a surpassÃ© la rÃ©gression logistique avec une **accuracy atteignant 100%** sur `make_moons`.
- Sur MNIST, les performances restent limitÃ©es sans optimisation mais dÃ©montrent la capacitÃ© du rÃ©seau Ã  traiter des donnÃ©es rÃ©elles.
- L'implÃ©mentation permet de bien visualiser l'effet du nombre d'Ã©poques sur l'apprentissage.


## ğŸ’¡ AmÃ©liorations possibles

- Ajouter plus de couches cachÃ©es (deep learning)
- Tester d'autres fonctions d'activation (ReLU, tanh)
- ImplÃ©menter la rÃ©gularisation (L2)
- Utiliser le mini-batch gradient descent
- Normaliser les donnÃ©es en entrÃ©e
- Passer Ã  une architecture CNN pour les images (MNIST)


---

